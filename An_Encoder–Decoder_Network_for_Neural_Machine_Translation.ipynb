{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/vhroV8ATdKhGhVn6Skiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulrahimzulfiqar/English-to-Spanish-Translation-Using-LSTM-Neural-Machine-Translation-/blob/main/An_Encoder%E2%80%93Decoder_Network_for_Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy5uANRmHNnu",
        "outputId": "8d11aedc-3405-4159-b26b-4042691a5a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "\u001b[1m2638744/2638744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Go.\tVe.\n",
            "Go.\tVete.\n",
            "Go.\tVaya.\n",
            "Go.\tVáyase.\n",
            "Hi.\tHola.\n",
            "Run!\t¡Corre!\n",
            "Run.\tCorred.\n",
            "Who?\t¿Quién?\n",
            "Fire!\t¡Fuego!\n",
            "Fire!\t¡Incendio!\n",
            "Fire!\t¡Disparad!\n",
            "Help!\t¡Ayuda!\n",
            "Help!\t¡Socorro! ¡Auxilio!\n",
            "Help!\t¡Auxilio!\n",
            "Jump!\t¡Salta!\n",
            "Jump.\tSalte.\n",
            "Stop!\t¡Parad!\n",
            "Stop!\t¡Para!\n",
            "Stop!\t¡Pare!\n",
            "Wait!\t¡Espera!\n",
            "Wait.\tEsperen.\n",
            "Go on.\tContinúa.\n",
            "Go on.\tContinúe.\n",
            "Hello!\tHola.\n",
            "I ran.\tCorrí.\n",
            "I ran.\tCorría.\n",
            "I try.\tLo intento.\n",
            "I won!\t¡He ganado!\n",
            "Oh no!\t¡Oh, no!\n",
            "Relax.\tTomátelo con soda.\n",
            "Smile.\tSonríe.\n",
            "Attack!\t¡Al ataque!\n",
            "Attack!\t¡Atacad!\n",
            "Ge\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Download manually to a known folder (not using TensorFlow's default)\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=url,\n",
        "    extract=False,        # ⛔ Don't extract automatically\n",
        "    cache_dir=\"/content\", # ✅ Use /content for Colab visibility\n",
        "    cache_subdir=\"\"\n",
        ")\n",
        "\n",
        "# ✅ Extract manually to a visible location\n",
        "extract_path = Path(\"/content/spa-eng-data\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# ✅ Read the file\n",
        "text_path = extract_path / \"spa-eng\" / \"spa.txt\"\n",
        "text = text_path.read_text(encoding='utf-8')\n",
        "\n",
        "# ✅ Show preview\n",
        "print(text[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs) # separates the pairs into 2 lists"
      ],
      "metadata": {
        "id": "luLZzhrnPaNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(sentences_en[i], \"=>\", sentences_es[i])"
      ],
      "metadata": {
        "id": "v9RUvjvRQR77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332cc495-dde8-4162-a6e5-cb6973e7b4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's raining cats and dogs. => Está lloviendo a cántaros.\n",
            "Tom left his briefcase on his desk. => Tom dejó la maleta sobre su escritorio.\n",
            "I went to China two years ago. => Fui a China hace dos años.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ],
      "metadata": {
        "id": "GWff04JPQYTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10],text_vec_layer_es.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "kJp68K6sbEtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1956b81-d1b7-4531-ccd3-6dafcd2f5b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['',\n",
              "  '[UNK]',\n",
              "  np.str_('the'),\n",
              "  np.str_('i'),\n",
              "  np.str_('to'),\n",
              "  np.str_('you'),\n",
              "  np.str_('tom'),\n",
              "  np.str_('a'),\n",
              "  np.str_('is'),\n",
              "  np.str_('he')],\n",
              " ['',\n",
              "  '[UNK]',\n",
              "  np.str_('startofseq'),\n",
              "  np.str_('endofseq'),\n",
              "  np.str_('de'),\n",
              "  np.str_('que'),\n",
              "  np.str_('a'),\n",
              "  np.str_('no'),\n",
              "  np.str_('tom'),\n",
              "  np.str_('la')])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ],
      "metadata": {
        "id": "2Oiwe2RvbVQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "c4bg-3_PeDCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "DggWChf1ePyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
      ],
      "metadata": {
        "id": "1cWRXtlEf-9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "opWrwKrvgd3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "Mkj1LVlbhVLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "A8c5ioTah5U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5302d5b-5c87-4f1a-a10a-e7d4abb003bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 24ms/step - accuracy: 0.0510 - loss: 3.5182 - val_accuracy: 0.0746 - val_loss: 2.1673\n",
            "Epoch 2/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 22ms/step - accuracy: 0.0787 - loss: 1.9551 - val_accuracy: 0.0870 - val_loss: 1.6514\n",
            "Epoch 3/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - accuracy: 0.0913 - loss: 1.4539 - val_accuracy: 0.0931 - val_loss: 1.4286\n",
            "Epoch 4/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 22ms/step - accuracy: 0.0990 - loss: 1.1887 - val_accuracy: 0.0957 - val_loss: 1.3330\n",
            "Epoch 5/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 24ms/step - accuracy: 0.1042 - loss: 1.0026 - val_accuracy: 0.0966 - val_loss: 1.2991\n",
            "Epoch 6/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 22ms/step - accuracy: 0.1088 - loss: 0.8614 - val_accuracy: 0.0971 - val_loss: 1.2946\n",
            "Epoch 7/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - accuracy: 0.1125 - loss: 0.7431 - val_accuracy: 0.0975 - val_loss: 1.3065\n",
            "Epoch 8/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - accuracy: 0.1162 - loss: 0.6365 - val_accuracy: 0.0971 - val_loss: 1.3432\n",
            "Epoch 9/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - accuracy: 0.1195 - loss: 0.5494 - val_accuracy: 0.0968 - val_loss: 1.3814\n",
            "Epoch 10/10\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 24ms/step - accuracy: 0.1222 - loss: 0.4777 - val_accuracy: 0.0966 - val_loss: 1.4345\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b0972a32690>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = tf.constant([sentence_en])  # raw English sentence string\n",
        "        X_dec = tf.constant([\"startofseq \" + translation])  # raw partial Spanish translation\n",
        "\n",
        "        y_proba = model.predict((X, X_dec))[0, word_idx]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "\n",
        "        translation += \" \" + predicted_word\n",
        "    return translation.strip()\n"
      ],
      "metadata": {
        "id": "g6nKSKg6iuOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40d59bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "92e5af5f-a877-4bc9-ad7a-53ef07755d8f"
      },
      "source": [
        "translate('i like car')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el carro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I like soccer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "XJv0uKlpViwq",
        "outputId": "6ee9938e-600b-4932-d50b-bc5c3ce77a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYAjsI_5Y7ej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}